#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Author: Daniel Rode


"""Find and delineate rock artifacts.

Take a list of hotspot points, buffer them into circular
plots, and segment the point cloud for each plot. Take some training data
and train a random forest model to distinguish between "rock" polygons
(segments) and "non-rock" polygons. Identify list of rock polygons for
each hotspot plot and save this list to a file.
"""


# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Import libraries
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

# Import standard libraries
import os
import sys
import hashlib
from io import StringIO
from pathlib import Path
from logging import ERROR
from functools import partial
from contextlib import redirect_stdout

# Import in-house libraries
from vogeler.stdlib import lsdir
from vogeler.stdlib import print2
from vogeler.stdlib import dispatch
from vogeler.stdlib import init_logger
from vogeler.extlib import catgdf

# Import external libraries
import shapely
import pyogrio
import numpy as np
import pandas as pd
import rasterio as rio
from rasterio.mask import mask
from pyproj.crs.crs import CRS
from geopandas import GeoDataFrame
from rasterstats import zonal_stats
from sklearn.ensemble import RandomForestClassifier

# Import R libraries
from vogeler.r import r
from vogeler.r import RS4
from vogeler.r import rlib
from vogeler.r import IntVector
from vogeler.r import rpy2_logger
from vogeler.r import R_NULL

r_base = rlib("base")  # Set `r_base` to call R's `base` package
sf = rlib("sf")  # Set `sf` to call R's `sf` package
lidr = rlib("lidR")  # Set `lidr` to call R's `lidR` package
terra = rlib("terra")  # Set `terra` to call R's `terra` package

rpy2_logger.setLevel(ERROR)  # Suppress R warning messages


# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Constants
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

EXE_NAME = sys.argv[0].split('/')[-1]  # This script's filename

# Text printed when user calls this script without proper parameters
HELP_TEXT = f"""Usage: {EXE_NAME}  LAS_PATH  NDVI_RAST_PATH  HOTSPOTS_SHP_PATH
  HOTSPOT_PLOT_RADIUS  MODEL_TRAINING_CSV  OUT_DIR"""

MAX_WORKERS = os.cpu_count()  # Number of CPU cores available on machine

# Point cloud import filters
CUSTOM_LIDAR_CODE_ROCK = 102
LIDAR_POINT_FILTER = (
    f"-keep_extended_class 0 1 2 3 4 5 9 {CUSTOM_LIDAR_CODE_ROCK} "
    "-drop_withheld"
)
LIDAR_ATTR_FILTER = "xyzirnc"  # See https://rdrr.io/cran/lidR/man/readLAS.html

# Filenames for output vectors and rasters
HOTSPOT_PLOT_BOUNDS_FILENAME = "plot.fgb"
BASIC_STATS_FILENAME = "basic_stats.fgb"
SHAPE_STATS_FILENAME = "shape_stats.fgb"
NDVI_STATS_FILENAME = "ndvi_stats.fgb"
ROCKS_GDF_FILENAME = "rocks.gpkg"
CHM_BUFFED_FILENAME = "chm_buffered.tif"

PLOT_PROCESSING_BUFF = 40  # To avoid edge effects

SEG_PARAMS = {
    # Parameters passed to point cloud segmentation algorithms
    "curve_start_x": 1,
    "curve_start_y": 1,
    "curve_x_range": 1,
    "curve_y_range": 1,
    "curve_rate": 1,
    "th_tree": 2,
    "th_seed": 0.01,
    "th_cr": 0.01,
    "max_cr": 99,
    "crown_geom": "concave",
}

# "Concaveness" of crown polygons (1.0 returns convex hull, 0.0 returns
# maximally concave hulls; see docs for R's `sf::st_concave_hull` function for
# more info)
CONCAVE_HULL_RATIO = 0.2

# Canopy height model (cursory one just used for finding rocks) spatial
# resolution
CHM_RES = 0.5

# Selection of zonal statistics to compute from NDVI (Normalized Difference
# Vegetation Index) imagery
NDVI_ZONAL_STATS = ["mean", "max", "min", "std"]

ROCK_CLASS = 1  # 0 = "not rock"
RF_X_VARS = [
    # Variable set from "no_i_drop_rec_ord2_reduced_plus_hor" model
    # (which as of 06 Dec 2024, is our best performing rock identification
    # model).
    "area",
    "ndvi_mean",
    "r_mean",
    "r_skew",
    "z_sd",
    "z_skew",
    "horizontality",
]

# Seed passed to random number generator
RF_SEED = 28  # "BH" = 28 in A1Z26 cipher


# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Functions
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

def hash_wkt(wkt: str) -> str:
    """Compute and return the SHA1 hash of the given WKT."""

    return hashlib.sha1(bytes(wkt, 'utf8')).hexdigest()


def get_chm(las: RS4, res=CHM_RES) -> RS4:
    """Generate a canopy height model for the given point cloud."""

    algo = lidr.pitfree(
        thresholds=IntVector((0, 2, 5, 10, 15)),
        max_edge=IntVector((5, 1)),
        subcircle=0,
        highest=True,
    )
    chm = lidr.rasterize_canopy(
        las,
        res=res,
        algorithm=algo,
        pkg="terra",
    )

    return chm


def segment_las(las: RS4, chm: RS4, params=SEG_PARAMS) -> RS4:
    """Segment trees (assign each point cloud return a crown ID)."""

    # Locate tree tops
    window_fn = r("""function(
        curve_start_x, curve_start_y, curve_x_range, curve_y_range, curve_rate
    ) {
        window_func = function(x) {
            # Set window size smaller for lower points and larger for higher
            # points (so that tall trees have a larger window than short trees)

            # https://www.mathsisfun.com/sets/function-transformations.html
            f = function(x) {
                return(-exp(-curve_rate * x) + 1)
            }

            curve_end_x = curve_start_x + curve_x_range
            curve_end_y = curve_start_y + curve_y_range
            curve_y_stretch =
                (curve_end_y - curve_start_y) / f(curve_end_x - curve_start_x)
            y = f(x - curve_start_x) * curve_y_stretch + curve_start_y

            y[x < curve_start_x] = curve_start_y
            y[x > curve_end_x] = curve_end_y

            return(y)
        }
        return(window_func)
    }""")(
        curve_start_x=params["curve_start_x"],
        curve_start_y=params["curve_start_y"],
        curve_x_range=params["curve_x_range"],
        curve_y_range=params["curve_y_range"],
        curve_rate=params["curve_rate"],
    )
    ttops = lidr.locate_trees(chm, lidr.lmf(ws=window_fn))

    # Setup Dalponte method
    dal = lidr.dalponte2016(
        chm,
        ttops,
        th_tree=params["th_tree"],
        th_seed=params["th_seed"],
        th_cr=params["th_cr"],
        max_cr=params["max_cr"],
    )

    # Segment crowns
    seg = lidr.segment_trees(las, dal)

    return seg


def get_seg_stats(las: RS4, crown_geom=SEG_PARAMS["crown_geom"]) -> RS4 | None:
    """Given a segmented point cloud, compute statistics for each crown."""

    # Define basic crown metrics function
    r.assign('las', las)
    crown_metrics_fn = r(f"""function(x, y, z, i, rn) {{
        # Crown geometry/shape/polygon
        shape = data.frame(x = x, y = y) |>
        sf::st_as_sf(coords = c("x", "y"), crs = las@crs) |>
        sf::st_union() |>
        sf::st_concave_hull(ratio = {CONCAVE_HULL_RATIO})

        # Function that computes skewness
        skew = function(a, a_mean) {{
            n = length(a)
            ans = (sum((a - a_mean)^3) / n) / (sum((a - a_mean)^2) / n)^(3 / 2)
            return(ans)
        }}

        # Function that computes kurtosis
        kurt = function(a, a_mean) {{
            n = length(a)
            ans = n * sum((a - a_mean)^4) / (sum((a - a_mean)^2)^2)
            return(ans)
        }}

        area = sf::st_area(shape)
        n_frst = sum(rn == 1)  # Count of first returns
        i_mean = mean(i)       # Mean lidar intensity
        r_mean = mean(rn)      # Mean return number
        x_mean = mean(x)       # Mean east/west position
        y_mean = mean(y)       # Mean north/south position
        z_mean = mean(z)       # Mean height
        metrics = list(
            # Use percentiles instead of max to account for outliers
            z_max  = max(z, na.rm = TRUE),  #------------# Max height
            z_p99  = quantile(z, c(.99), na.rm = TRUE),  # 99th ht. percentile
            z_p95  = quantile(z, c(.95), na.rm = TRUE),  # 95th ht. percentile
            z_mean = mean(z),  #-------------------------# Mean height
            z_sd   = sd(z),  #---------------------------# Height variability
            i_max  = max(i, na.rm = TRUE),  #------------# Max intensity
            i_p99  = quantile(i, c(.99), na.rm = TRUE),  # 99th intensity %ile
            i_p95  = quantile(i, c(.95), na.rm = TRUE),  # 95th intensity %ile
            i_mean = i_mean,  #--------------------------# Mean intensity
            i_sd   = sd(i),  #---------------------------# Intensity variab.
            n_all  = length(x),  #-----------------------# Number of returns
            n_frst = n_frst,  #-----------------------# Number of first returns
            pa_rat = sf::st_perimeter(shape) / area,  # Perimeter to area ratio
            r_mean = r_mean,  #-----------------------# Mean number of returns
            x_mean = x_mean,  #------------# Mean east/west position
            y_mean = y_mean,  #------------# Mean north/south position
            i_skew = skew(i, i_mean),  #---# Intensity skewness
            i_kurt = kurt(i, i_mean),  #---# Intensity kurtosis
            x_skew = skew(x, x_mean),  #---# East/west position skewness
            x_kurt = kurt(x, x_mean),  #---# East/west position kurtosis
            y_skew = skew(y, y_mean),  #---# North/south position skewness
            y_kurt = kurt(y, y_mean),  #---# North/south position kurtosis
            z_skew = skew(z, z_mean),  #---# Height skewness
            z_kurt = kurt(z, z_mean),  #---# Height kurtosis
            r_skew = skew(rn, r_mean),  #--# Return number skewness
            r_kurt = kurt(rn, r_mean),  #--# Return number kurtosis
            x_ent  = lidR::entropy(x),  #--# East/west position entropy
            y_ent  = lidR::entropy(y),  #--# North/south position entropy
            z_ent  = lidR::entropy(z),  #--# Height entropy
            i_ent  = lidR::entropy(i),  #--# Intensity entropy
            r_ent  = lidR::entropy(rn),  #-# Return number entropy
            area   = area  #---------------# "Crown" (segment) area
        )
        return(metrics)
    }}""")
    r.assign('crown_metrics_fn', crown_metrics_fn)

    # Compute basic crown metrics
    with redirect_stdout(StringIO()):
        crown_stats = lidr.crown_metrics(
            las,
            func=r("""
                ~crown_metrics_fn(x=X, y=Y, z=Z, i=Intensity, rn=ReturnNumber)
            """),
            geom=crown_geom,
        )

    if crown_stats is R_NULL:
        return None

    return crown_stats


def get_seg_shape_stats(las: RS4, crown_geom=SEG_PARAMS["crown_geom"]) -> RS4:
    """Compute shape statistics for each crown in segmented point cloud."""

    crown_stats = lidr.crown_metrics(
        las,
        func=r("~lidR::stdshapemetrics(x=X, y=Y, z=Z)"),
        geom=crown_geom,
    )

    return crown_stats


def sf_sel_geoms_within(bounds: RS4, geoms: RS4) -> RS4:
    """Select polygons encompassed by a given polygon.

    Given a "bounds" polygon, return all features from "geoms" that are
    fully within the perimeter of "bounds".
    """

    fn = r("""function(bounds, geoms) {
        filter = sf::st_within(geoms, bounds, sparse=FALSE)
        return(geoms[filter, ])
    }""")

    return fn(bounds, geoms)


def mark_plot_empty(
    plot_dir: Path,
    note="No crowns found in this plot",
) -> None:
    """Mark a plot directory as having no crowns.

    Create a text file within the given plot directory to note that no
    crowns were found in that plot.
    """

    empty_txt_file = plot_dir / "EMPTY"
    with empty_txt_file.open('w') as f:
        f.write(note)


def len_sf(sf_collection: RS4) -> int:
    """Return count of features in sf object."""

    return r_base.nrow(sf_collection)[0]


def hotspot_plot_worker(
    plot_wkt: str,
    plot_crs: CRS,
    ctg_path: Path,
    ndvi_path: Path,
    out_dir: Path,
    proc_buff=PLOT_PROCESSING_BUFF,
) -> None:
    """Process a plot: segment then crown metrics.

    For the given plot, segment into polygons, then calculate LiDAR crown
    metrics for each segment.
    """

    # Ensure directory for hotspot plot exists
    plot_id = hash_wkt(plot_wkt)
    plot_dir = out_dir / plot_id
    plot_dir.mkdir(exist_ok=True)

    # Save plot polygon and then buffer it to avoid edge effects during
    # processing
    plot = shapely.from_wkt(plot_wkt)
    (
        GeoDataFrame(geometry=[plot], crs=plot_crs)
        .to_file(plot_dir / HOTSPOT_PLOT_BOUNDS_FILENAME)
    )
    plot_buffed = plot.buffer(proc_buff)

    # If job was already ran, abort
    if (
        (plot_dir / BASIC_STATS_FILENAME).exists()
        and
        (plot_dir / SHAPE_STATS_FILENAME).exists()
        and
        (plot_dir / NDVI_STATS_FILENAME).exists()
    ) or (plot_dir / "EMPTY").exists():
        log.info("Job already ran (skipping): %s", plot_id)
        return

    # Load point cloud (with buffer)
    ctg = lidr.readLAScatalog(
        str(ctg_path),
        progress=False,
        select=LIDAR_ATTR_FILTER,
        filter=LIDAR_POINT_FILTER,
    )
    bounds = sf.st_as_sfc(plot_buffed.wkt, crs=plot_crs.to_wkt())
    las = lidr.clip_roi(ctg, bounds)

    # Generate buffered CHM for plot and save to file
    chm = get_chm(las)
    terra.writeRaster(
        chm,
        str(plot_dir / CHM_BUFFED_FILENAME),
        filetype="GTiff",
        overwrite=True,
    )

    # Segment plot
    seg = segment_las(las, chm)

    # Compute and save statistics for each segment ("crown")
    seg_stats = get_seg_stats(seg)
    if seg_stats is None:
        log.info("No crowns found: %s", plot_id)
        mark_plot_empty(plot_dir)
        return
    seg_stats = sf_sel_geoms_within(  # Trim off processing buffer
        sf.st_as_sfc(plot.wkt, crs=plot_crs.to_wkt()),
        seg_stats,
    )
    if len_sf(seg_stats) == 0:
        log.info("Only crowns found were in buffer: %s", plot_id)
        mark_plot_empty(plot_dir, note="Only crowns found were in buffer")
        return
    sf.st_write(seg_stats, str(plot_dir / BASIC_STATS_FILENAME), append=False)

    seg_shape_stats = get_seg_shape_stats(seg)
    seg_shape_stats = sf_sel_geoms_within(  # Trim off processing buffer
        sf.st_as_sfc(plot.wkt, crs=plot_crs.to_wkt()),
        seg_shape_stats,
    )
    sf.st_write(
        seg_shape_stats, str(plot_dir / SHAPE_STATS_FILENAME), append=False,
    )

    # Calculate NDVI stats for each segment

    # Import output from clip-segment-stats.r script
    df = pyogrio.read_dataframe(plot_dir / BASIC_STATS_FILENAME)

    # Import NDVI raster
    with rio.open(ndvi_path) as f:
        shapes = df.to_crs(f.crs.wkt)["geometry"]
        rast, affine = mask(f, shapes, crop=True)
    band = rast[0]  # Select first raster band

    # Calculate zonal statistics
    zs = zonal_stats(
        shapes,
        band,
        affine=affine,
        stats=NDVI_ZONAL_STATS,
        nodata=np.nan,
    )

    # Join zonal stats to original polygons (not the reprojected ones
    # generated by zonal_stats function) and save to file
    zs = GeoDataFrame(
        pd.concat([df["geometry"], pd.DataFrame.from_dict(zs)], axis=1)
    )
    zs.to_file(plot_dir / NDVI_STATS_FILENAME)


def get_plot_data(
    plot_dir: Path,
    basic_stats_filename=BASIC_STATS_FILENAME,
    shape_stats_filename=SHAPE_STATS_FILENAME,
    ndvi_stats_filename=NDVI_STATS_FILENAME,
) -> GeoDataFrame:
    """Combine multiple stat sets into single dataframe for a given plot.

    Join LiDAR basic stats with LiDAR shape stats and NDVI stats for a given
    plot.
    """

    # If plot is empty (has no stats), return empty GeoDataFrame
    if (plot_dir / "EMPTY").exists():
        return GeoDataFrame()

    # Import shapefiles
    lidar_basic_stats = pyogrio.read_dataframe(plot_dir / basic_stats_filename)
    lidar_shape_stats = pyogrio.read_dataframe(plot_dir / shape_stats_filename)
    ndvi_stats = (
        pyogrio
        .read_dataframe(plot_dir / ndvi_stats_filename)
        .rename(
            # Prepend "ndvi_" to col names
            lambda x: x if x == "geometry" else "ndvi_" + x,
            axis="columns",
        )
    )

    # Verify all shapefiles have the same CRS
    assert lidar_basic_stats.crs == lidar_shape_stats.crs == ndvi_stats.crs

    # Merge stats data into a single dataframe
    def chain_merge_df(df_list):
        df_iter = iter(df_list)
        right = next(df_iter)
        for left in df_iter:
            right = right.merge(
                left,
                on="geometry",
                how="inner",
                suffixes=(None, "_dup"),
            )
        return right
    df = chain_merge_df([lidar_basic_stats, lidar_shape_stats, ndvi_stats])

    return GeoDataFrame(df, crs=lidar_basic_stats.crs)


def train_rf_model(
    training_csv_path: Path, x_cols=RF_X_VARS, y_col="class",
) -> RandomForestClassifier:
    """Train random forest model to identify rock vs not-rock polygons."""

    # Import, format, and shape training data
    train_df = pd.read_csv(training_csv_path)
    train_df = train_df[train_df[y_col].notna()]
    train_df[y_col] = train_df[y_col].astype(int)

    # Train random forest classifier
    rf = RandomForestClassifier(random_state=RF_SEED)
    rf.fit(train_df[x_cols], train_df[y_col])

    return rf


def apply_rf_model(
    df: GeoDataFrame, rf_model: RandomForestClassifier, x_vars=RF_X_VARS,
) -> GeoDataFrame:
    """Use random forest model to identify rock vs not-rock polygons."""

    # Apply model to make class predictions on plot data
    df['pred'] = rf_model.predict(df[x_vars])

    # Save certainty data (how convinced the model is that a classification is
    # correct)
    df['pred_proba'] = rf_model.predict_proba(df[x_vars]).max(axis=1)

    return df


def get_hotspot_plots(hotspots_df: GeoDataFrame, rad: float) -> GeoDataFrame:
    """Convert hotspot pixels into hotspot polygon plots.

    Take list of hotspots points, buffer them, dissolve into a single
    multi-polygon, explode the multi-polygon into individual non-touching
    polygons. Give each plot an ID.
    """

    # Buffer points and dissolve overlapping plots
    geom = hotspots_df.buffer(
        rad,
        cap_style='round',
        join_style='round',
    ).union_all()
    hotspot_plots = GeoDataFrame(
        geometry=[geom],
        crs=hotspots_df.crs,
    ).explode(ignore_index=True)

    # SHA1 hash each plot polygon WKT and use that as plot ID
    hotspot_plots['plot_id'] = (
        hotspot_plots['geometry']
        .to_wkt()
        .apply(hash_wkt)
    )

    return hotspot_plots


def gen_rocks_gdf(
    plot_paths: list[Path], rf_model: RandomForestClassifier,
) -> GeoDataFrame:
    """Generate dataframe of rock polygons for entire study area.

    Use the given rock detection model to search for rocks in the given list
    of plots, and return all rock "crowns" as a dataframe of polygons.
    """

    cols = ['geometry', 'pred_proba']

    # Import hotspot plot data (and discard plots with no stats/crowns)
    df = catgdf(filter(
        lambda x: len(x) > 0,
        (get_plot_data(p) for p in plot_paths),
    ))

    # If no hotspots exist, return empty rock dataframe
    if len(df) == 0:
        return GeoDataFrame(columns=cols)

    # Classify polygons as rock or not-rock
    df = apply_rf_model(df, rf_model)

    # Drop all non-rock polygons
    df = df[df['pred'] == ROCK_CLASS]

    # Return rock polygons
    return df[cols]


# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
# -----------------------------------------------------------------------------

def main():
    # Parse command line arguments
    max_workers = MAX_WORKERS
    pos_args = []
    args = iter(sys.argv[1:])
    for a in args:
        if (not a.startswith('-')) or (a == '--'):
            pos_args += [a] + list(args)
            break
        try:
            match a:
                case '--max-workers':
                    max_workers = int(next(args))
                case _:
                    print2("error: Invalid flag", a)
                    sys.exit(1)
        except StopIteration:
            print2("error: Flag requires value:", a)
            sys.exit(1)
        except ValueError:
            print2("error: Invalid value for flag:", a)
            sys.exit(1)

    try:
        ctg_path = Path(pos_args[0])
        ndvi_path = Path(pos_args[1])
        hotspots_shp = Path(pos_args[2])
        hotspot_plot_radius = float(pos_args[3])
        model_training_csv_path = Path(pos_args[4])
        out_dir = Path(pos_args[5])
    except IndexError:
        print2(HELP_TEXT)
        sys.exit(1)

    # Ensure source path(s) exist
    for p in (hotspots_shp,):
        if not p.exists():
            print2("error: Path does not exist:", p)
            sys.exit(1)

    # Setup logging
    global log
    log = init_logger()

    # Print start time
    log.info("Starting...")

    log.warning(
        "This script expects that given point clouds are already height"
        " normalized"
    )

    # Find hotspots (locations where the DTM likely needs correction)
    log.info("Loading hotspots: %s", hotspots_shp)
    hotspots = pyogrio.read_dataframe(hotspots_shp)

    # Save map of all hotspot plots
    log.info("Generating hotspot plots from points...")
    hotspot_plots = get_hotspot_plots(hotspots, hotspot_plot_radius)

    # Segment point clouds for each hotspot plot and calculate stats for
    # each "crown" (segment) polygon
    log.info("Segmenting hotspot plots and calculating 'crown' stats...")
    jobs = hotspot_plots['geometry'].to_wkt()
    plots_dir = out_dir / "plots"
    plots_dir.mkdir(exist_ok=True, parents=True)
    fn = partial(
        hotspot_plot_worker,
        plot_crs=hotspot_plots.crs,
        ctg_path=ctg_path,
        ndvi_path=ndvi_path,
        out_dir=plots_dir,
    )
    for job, _ in dispatch(jobs, fn, max_workers=max_workers):
        log.info("Job finished: %s", hash_wkt(job))

    # Import training data and train random forest model
    log.info("Training random forest model to identify rocks...")
    rf = train_rf_model(
        model_training_csv_path,
        x_cols=RF_X_VARS,
        y_col="class",
    )

    # Save map (polygon collection) of all found rock "crowns"
    log.info("Locating and saving map of rocks in all hotspot areas...")
    rock_map = gen_rocks_gdf(lsdir(plots_dir), rf)
    rock_map.to_file(out_dir / ROCKS_GDF_FILENAME)


if __name__ == '__main__':
    main()
